<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: deploystudio | Sean Kaiser (dot) com]]></title>
  <link href="http://seankaiser.com/blog/categories/deploystudio/atom.xml" rel="self"/>
  <link href="http://seankaiser.com/"/>
  <updated>2013-05-12T02:10:32-04:00</updated>
  <id>http://seankaiser.com/</id>
  <author>
    <name><![CDATA[Sean Kaiser]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The Big Project- Preparing the machines for deployment with DeployStudio]]></title>
    <link href="http://seankaiser.com/blog/2013/01/31/the-big-project-preparing-the-machines-for-de/"/>
    <updated>2013-01-31T06:00:00-05:00</updated>
    <id>http://seankaiser.com/blog/2013/01/31/the-big-project-preparing-the-machines-for-de</id>
    <content type="html"><![CDATA[<h2>Previously on "The Big Project"</h2>

<p>In <a href="/blog/2013/01/15/were-doing-what-and-on-what-timeline/">part 1</a> of this series, I provided an overview of what I now call "The Big Project." <a href="/blog/2013/01/22/the-big-project-setting-the-stage/">Part 2</a> talked about the importance of inventorying. This article is the first of a series of more detailed technical articles describing various aspects and the tools we used to pull off this project. FIrst up...</p>

<h2>DeployStudio</h2>

<p>We've been using DeployStudio for several years to help us image machines. In the (relatively distant) past, we restored full monolithic images to machines, new or old. Over the past almost two years, we've switched to the thin imaging model. Thin imaging is a method of preserving the contents of a machine's hard drive and deploying either site specific applications or deployment tools to facilitate the installation of such applications and other files in an effort to minimize the amount of time needed to get a machine ready for a user. In our case, we deploy our deployment tools (puppet and munki) and some basic settings. This cuts down on the deployment cost, in both time and bits traveling across the network.</p>

<p>In situations where we need to reimage a machine, we have a workflow that lays down an InstaDMG created vanilla image and then runs the normal thin imaging workflow.</p>

<!-- more -->


<h2>The thin imaging workflow</h2>

<p>When we "Northmont-ize a new machine" (the name of our thin imaging workflow), DeployStudio performs the initial basic setup fo our machines. Everything is rather basic at this point. The workflow does the following:</p>

<ul>
<li>set the time server to our internal NTP server. (This is the only non-automated step in the workflow. I didn't automate this step to let the technician confirm that the correct drive is selected as the target volume. All other steps are automated and use the previous task target option for the target volume.)</li>
<li>set the computer's name via a query to our help desk</li>
<li>perform an anonymous bind the machine to our open directory server</li>
<li>install puppet, facter, a basic puppet.conf file, a tweaked ruby puppet wrapper that I copied from Gary Larizza way back when he was still in the K12 education space at Huron, and a launchdaemon to keep things running on a schedule</li>
<li>create our default local user set (the users are actually payloadless packages created from InstaDMG's <a href="http://code.google.com/p/instadmg/source/browse/trunk/AddOns/createUser/archived/CreateLionUser-README.txt">CreateLionUser</a>.) We create two admin accounts, one for the district tech staff to use and one for the in-building tech support staff to use. We also have a passwordless "classroom" account so that users can access the local machine in case the network or server (either file server or open directory infrastructure) is down.</li>
<li>install munki</li>
<li>skip Apple's setup assistant that runs for new machines and enable the ARD agent</li>
<li>set munki's client_identifier via a query to our help desk</li>
<li>set softwareupdate's CatalogURL to point to the central reposado server and, based on information gathered via a query to our help desk, configure it to use the test catalog</li>
<li>join the computer to the appropriate wifi network as specified in the help desk (if one is specified. Since a majority our new computers are MacBook Pros, they all have a wifi network specified. The new iMacs and most of the relocated machines are all hardwired, so they don't have a network specified, and the script just exits appropriately.)</li>
<li>perform a non-destructive partition of the hard drive into two partitions (after checking that the drive only has one partition.) The first partition is set to 250GB, with the second partition using the remaining space (which has to be calculated since the diskutil command doesn't know how to "use remaining space".) The script then grabs the UUID for the new second partition and adds it to /etc/fstab to be mounted at /Users (after checking that this information isn't already in the file.) It also creates a new Shared folder to match the one that's in the /Users folder by default on an OS X machine. Having this partition in place allows the users of the machine to have a space that's safe from future reimaging. Our teaching staff are using portable home directories (I know... bad word) so their local synchronized home folder sits on this "safe" partition.</li>
<li>touches /Users/Shared/.com.googlecode.munki.checkandinstallatstartup to <a href="http://code.google.com/p/munki/wiki/BootstrappingWithMunki">bootstrap munki</a> (make munki run immediately upon boot instead of on its regular schedule.)</li>
<li>reboot the machine (I have a reboot step because I've configured DeployStudio not to reboot after a workflow runs so we can run multiple workflows if necessary. The drawback of this is that computers never appear to finish in the Activity area of DeployStudio.)</li>
</ul>


<p>All of our workflows run as "postponed installations". Is this necessary? For some steps, such as installing packages, it seems to work better. Since I'm doing a postponed install for some things, I decided to do it for all steps.</p>

<h2>Full reimage workflow</h2>

<p>If we do actually deploy a new image, it's usually because something has gone wrong on the machine or we've simply replaced the hard drive. I've used InstaDMG to create a vanila 10.6, 10.7, and 10.8 image. Since we have 6 elementary schools, a middle school, a high school, a service center (where my department's offices are), and the administrative office, each with their own LAN and only a 25 mbps WAN link back to the datacenter, I've implemented a modified version of a strategy I discovered a couple of years ago to have a <a href="http://www.deploystudio.com/Forums/viewtopic.php?pid=2614">central DeployStudio server with local images repositories</a>. (I know DeployStudio now offers a synchronization process, but I've never looked into it. This process works well for us. And, remember from part 1, one of the steps I had to complete to start this project was to update DeployStudio and its NetBoot image... I doubt that prior to a month and a half ago that the version we were running offered synchronization.)</p>

<p>So, our complete reimage workflow looks like this:</p>

<ul>
<li>run the local repository mount script</li>
<li>restore the vanilla image</li>
<li>run the northmont-ize workflow described above</li>
</ul>


<h2>The logistics of prepping 1200+ machines</h2>

<p>We wanted our machines to be usable as soon as we deployed them so as to cut down on the amount of time we were in the schools. (In one case, we actually replaced a school's computers as the students cycled through lunch, which at that school's was about an hour and a half.)</p>

<p>Since our offices are on the second floor in our building, and although our server room has a garage door that allows our warehouseman to forklift pallets of equipment through, we had no intention of moving 240+ 5-pack boxes of MacBook Pros any more than we really needed to. We commandeered a couple of tables near the staff mailboxes downstairs between our storage "cage" and the warehouse, dropped a couple of extension cords and a patch cord (offering a gigabit connection) from the office space above, and set up an imaging bench with an HP switch, two power strips, and 10 Magsafe power adapters. One of my coworkers had the idea of taping the power and patch cords to the tables so they didn't slide around. Initially I wasn't sure this was necessary, but it turned out to be brilliant.</p>

<p>Since we could get 10 computers going at a time, and get those 10 computers booted into DeployStudio, run the workflow, reboot to go through the firstboot process, reboot a second time, let puppet and munki run to install our settings and applications (which will be detailed in subsequent articles), then shut the computers down to rebox them, all in about 15-20 minutes per group. While one batch was running, we'd unbox the next batch and have them ready to move into place when the previous batch was finished.</p>

<p><strong>Think about that... every 15-20 minutes, we had 10 computers ready to be deployed, with our applications, our users, our settings, etc.</strong> Repeat that process 120+ times, and you can understand how I was able to successfully Northmont-ize an iMac whose display was DOA. After a while, I could do that process in my sleep, knowing about how long to wait for each step of the process, how many down arrows to press to get to our workflow, etc. I did get a bit excited (probably overly so) when we heard the iMac reboot after the workflow ran, then reboot again a couple of minutes later when the first boot processes finished.</p>

<p>Another benefit of thin imaging and using tools like puppet and munki is that as things change, things need to be added, etc., you've got a system in place that will handle that, automatically. You don't need to rebuild your image.</p>

<h2>What's coming up</h2>

<p>I'll have dedicated articles on our puppet and munki configurations, a brief discussion on how I'm using luggage to package printer installers and repackage applications that don't come with good installers, some tips on inventorying machines, and some scripts that I've written to tie everything together, and finally reposado, in that general order.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Big Project- We're doing what, and on what timeline?!?]]></title>
    <link href="http://seankaiser.com/blog/2013/01/15/were-doing-what-and-on-what-timeline/"/>
    <updated>2013-01-15T12:30:00-05:00</updated>
    <id>http://seankaiser.com/blog/2013/01/15/were-doing-what-and-on-what-timeline</id>
    <content type="html"><![CDATA[<h2>The project</h2>

<p>Deploy 1170 MacBook Pros (later amended to 1250), 60 iMacs, 214 Apple TVs, 106 (full sized) new iPads, and 30 iPad minis. Relocate 100 or so "newer" (one or two year old) machines into different locations, either in different classrooms in the same building or to a different building. Retrieve nearly 1700 old machines (iBooks, eMacs, iMacs, Mac Minis, MacBooks) back to the warehouse for disposal. And do all of this in roughly 3 weeks (once the computers started arriving.)</p>

<h2>The "problems"</h2>

<p>Under any normal circumstances, this would be a <strong>huge</strong> summer project that we'd start planning around this time of the year. But this project was on the fast track. For various reasons, we were going to do this during the school year. The vast majority of devices would be ordered in two waves, the first in mid-November and the second a week or two later (with the additional 80 MacBook Pros ordered in early December), and we'd start deploying machines ASAP because of limited warehouse space. And, we'd try to have everything deployed before winter break was over. Although I was involved with the later stages of deciding what computers we were buying and in what quantities, most of the rest of our team hadn't been involved, and didn't even know about the project until around the time that the first order went in. (We didn't want too many people knowing about the project until it had official board approval.) Our team (our supervisor, 3 technicians, our administrative assistant and myself) had our work cut out for us.</p>

<!-- more -->


<p>Inventorying all of the new equipment, then physically removing the old computers and placing new 13" MacBook Pros would be a relatively easy process. The real challenge was getting our deployment tools updated and in place in order to get 1400+ computers ready to deploy (or redeploy.) We'd already bought into the "thin imaging" mindset (adding management tools to a new machine and using those tools to install applications, etc., versus building a whole new image with necessary applications included) via <a href="http://www.deploystudio.com/Home.html">deploystudio</a>. We had been using <a href="http://puppetlabs.com/puppet/what-is-puppet/">puppet</a> to do our software installs, but we hadn't been very diligent in keeping the software that was installed on existing machines updated (we had only recently pushed Firefox 15 out to machines that had been running 3.6.28, for example.)</p>

<p>I've been to several conferences and learned about <a href="http://code.google.com/p/munki/">munki</a>, and wanted to start using it, but had never taken the time to get something set up beyond using puppet to install munki on a test machine. Now I had to get a fully functioning munki system set up, and I had to do it quickly. I had to change our puppet configuration to manage the state of the computers (things like ensuring remote login was enabled, certain users existed on the computers, etc.) and move software installation over to munki. I also had to change the scripts that I had written for puppet to look up information from our help desk to return valid data for munki (while still returning valid data for puppet.) Based on the responses on a recent staff survey, we wanted to give staff the ability to install and update software without needing help from someone in our department. We wanted to leverage munki's optional software feature to make software available and let the end users decide if it was something they want to use. Because munki doesn't require a password to do installations/updates, our end users will now be able to do their own installations and updates (at least as much as we make available to them.)</p>

<p>Since we hadn't purchased very many computers recently, our deploystudio netboot set was woefully out of date (it was still built on Mac OS X 10.6.4, I believe), so it wouldn't be able to boot any of these new machines. I'd have to get that updated, too. Fortunately, we had a few MacBook Pros from the same hardware family so I was able to get a jump start on that and not have to wait until the first order arrived and it's not a big deal to get a base deploystudio netboot image built (or updated.) The real work is in the workflows anyway, and I'd have to update those to start installing munki and some other stuff.</p>

<p>We don't want to fall behind on updates again, so in addition to using munki to handle application updates, I'm working to get <a href="https://github.com/wdas/reposado">reposado</a> set up to handle Apple software updates. This is the one part that isn't quite done, mainly because of how I want things to work.</p>

<p>There were some other situations that required that some custom code be written. One of these was that some of the computers (the 80 that were added at the end of the project) needed VPN connections to be configured. In an effort to be as hands off as possible in the configuration process, I needed to have a way to do that.</p>

<p>So that was the project. No big deal... ha. This was by far the biggest project our department had ever undertaken in the 12+ years I've been here. And we had to be successful, or we'd have everyone questioning why we were doing this during the school year.</p>

<p>Over the next few posts, I'll describe how I configured deploystudio, puppet, and munki to work for us. I'll throw in some <a href="https://github.com/unixorn/luggage">luggage</a> discussion since I'm using munki to deploy printers and needed to repackage some applications as well. Once it's done, I'll describe our reposado configuration as well. So stay tuned... this was a fun project (I can say it now that it's essentially done and we're starting to get our heads back above water) and I'm looking forward to sharing our experience.</p>
]]></content>
  </entry>
  
</feed>
