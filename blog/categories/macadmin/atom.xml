<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: macadmin | Sean Kaiser (dot) com]]></title>
  <link href="http://seankaiser.com/blog/categories/macadmin/atom.xml" rel="self"/>
  <link href="http://seankaiser.com/"/>
  <updated>2013-12-16T22:59:46-05:00</updated>
  <id>http://seankaiser.com/</id>
  <author>
    <name><![CDATA[Sean Kaiser]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AutoPkg change notifications]]></title>
    <link href="http://seankaiser.com/blog/2013/12/16/autopkg-change-notifications/"/>
    <updated>2013-12-16T22:01:00-05:00</updated>
    <id>http://seankaiser.com/blog/2013/12/16/autopkg-change-notifications</id>
    <content type="html"><![CDATA[<h2>AutoPkg change notifications</h2>

<h1>Background</h1>

<p>We use <a href="https://github.com/autopkg/autopkg">AutoPkg</a> to automatically download (and process into <a href="http://code.google.com/p/munki/">munki</a>) updates to our commonly installed applications and internet plugins. One common practice is to run AutoPkg via <a href="http://jenkins-ci.org">JenkinsCI</a>, but I have not taken the time to install and configure Jenkins, so I just run AutoPkg via a <strike>cron</strike> launch daemon script.</p>

<p>Because I didn't want to have to check munki periodically to see if AutoPkg had downloaded anything, I wrote a wrapper script which emailed me the output from each AutoPkg run, which in our environment happens at the top of each hour. After a weekend of getting hourly emails (which I had to browse to check for any updates), I decided there had to be a better way to be notified of AutoPkg's work.</p>

<!-- more -->


<h1>Behold the power of diff</h1>

<p>As I started thinking about my approach to this problem, I immediately thought that I wanted to be notified only when something was different from a 'nothing downloaded, packaged  or imported' (the actual output from AutoPkg when nothing has been done) run. And how better to do that than to compare the output from the run against the output when 'nothing (was) downloaded, packaged or imported.'</p>

<p>My solution was to save the output of a AutoPkg run when nothing was done, and on each subsequent AutoPkg run, check that run's output against the saved output. If there are differences between these two files (because of either an update being downloaded, packaged or imported, or a download error), the script emails me the current AutoPkg run's output. This way when I get the email from the script, I know there's something that likely needs my attention.</p>

<h2>Customizing the script</h2>

<p>I've modified the script somewhat from my initial version, making it easier for other admins to customize for their environment. As you can see, there are three customization variables (<code>recipe_list</code>, <code>mail_recipient</code>, and <code>autopkg_user</code>) in the script. The only changes you'll need to make to the script are in those three lines.</p>

<h1>recipe_list</h1>

<p>You'll want to add whatever recipes you're feeding to AutoPkg on this line. If you're using munki, you'll want to be sure to include MakeCatalogs.munki at the end of the list, of course. In the example, I'm only using the AdobeFlashPlayer.munki and MakeCatalogs.munki recipes, indicating that I want to download and import any Flash updates, then rebuild the munki catalogs.</p>

<h1>mail_recipient</h1>

<p>This is the email address you want the change notification to be sent to.</p>

<h1>autopkg_user</h1>

<p>This is the local user that will be running the autopkg-wrapper script. The default (nothing changed) log will be stored in this user's Documents/autopkg folder.</p>

<h2>Installation and initial configuration</h2>

<p>Download the <a href="https://github.com/seankaiser/automation-scripts/blob/master/autopkg/autopkg-wrapper.sh">script</a> and install it in <code>/usr/local/bin</code>.</p>

<p>As you're setting things up, you'll want to run this script manually, and you'll be prompted to run the script with the <code>initialize</code> argument, which will tell the script to run autopkg with your recipe list twice, the second of which will save the output information to the default log location for later reference. You should also manually run the script with the <code>initialize</code> option if you change the recipe list, since that will change the output.</p>

<p>Once you've run the script manually, you're ready to set the script to run autoatically, either via cron or launchd. Obviously Apple wants you to run things via launchd, but it will work just fine as a cron job. An <a href="https://github.com/seankaiser/automation-scripts/blob/master/autopkg/autopkg-wrapper.sh">example launchd plist file</a> is available in my github repository. You'll need to modify the plist to meet your needs. It assumes the following:
 - the <code>autopkg-wrapper.sh</code> script lives in <code>/usr/local/bin</code>,
 - the user who runs the script is <code>autopkg</code>,
 - you want the script to run hourly, at the top of the hour.</p>

<p>If you're using the launchd plist, be sure to load it via something like <code>launchctl load /Library/LaunchDaemons/com.example.autopkg-wrapper.plist</code> (or reboot your machine, which will force the plist to be loaded on boot.)</p>

<h2>What else?</h2>

<p>That's pretty much it. Modify the three variables, load the launchd plist (or add to cron), and wait for the email notifications when AutoPkg finds and processes updates for you.</p>

<h2>The code</h2>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/sh
</span><span class='line'>
</span><span class='line'># autopkg automation script which, when run with no arguments, checks current run's output against a default output and sends the output to a user if there are differences
</span><span class='line'>
</span><span class='line'># adjust the following variables for your particular configuration
</span><span class='line'>recipe_list="AdobeFlashPlayer.munki MakeCatalogs.munki"
</span><span class='line'>mail_recipient="you@yourdomain.net"
</span><span class='line'>autopkg_user="autopkg"
</span><span class='line'>
</span><span class='line'># don't change anything below this line
</span><span class='line'>
</span><span class='line'># define logger behavior
</span><span class='line'>logger="/usr/bin/logger -t autopkg-wrapper"
</span><span class='line'>user_home_dir=`dscl . -read /Users/${autopkg_user} NFSHomeDirectory | awk '{ print $2 }'`
</span><span class='line'>
</span><span class='line'># run autopkg
</span><span class='line'>if [ "${1}" == "help" ]; then
</span><span class='line'>  # show some help with regards to initialization option
</span><span class='line'>  echo "usage: ${0} [initialize]"
</span><span class='line'>  echo "(initializes a new default log for notification checking)"
</span><span class='line'>  exit 0
</span><span class='line'>
</span><span class='line'>elif [ ! -f "${user_home_dir}"/Documents/autopkg/autopkg.out ]; then
</span><span class='line'>  # default log doesn't exist, so tell user to run this script in initialization mode and exit
</span><span class='line'>  echo "ERROR: default log does not exist, please run this script with initialize argument to initialize the log"
</span><span class='line'>  exit -1
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>elif [ "${1}" == "initialize" ]; then
</span><span class='line'>  # initialize default log for automated run to check against for notification if things have changed
</span><span class='line'>  $logger "starting autopkg to initialize a new default output log"
</span><span class='line'>
</span><span class='line'>  echo "recipe list: ${recipe_list}"
</span><span class='line'>  echo "autopkg user: ${autopkg_user}"
</span><span class='line'>  echo "user home dir: ${user_home_dir}"
</span><span class='line'>
</span><span class='line'>  # make sure autopkg folder exists in autopkg_user's Documents folder
</span><span class='line'>  if [ ! -d "${user_home_dir}"/Documents/autopkg ]; then
</span><span class='line'>    /bin/mkdir -p "${user_home_dir}"/Documents/autopkg
</span><span class='line'>  fi
</span><span class='line'>
</span><span class='line'>  # run autopkg twice, once to get any updates and the second to get a log indicating nothing changed
</span><span class='line'>  $logger "autopkg initial run to temporary log location"
</span><span class='line'>  echo "for this autopkg run, output will be shown"
</span><span class='line'>  /usr/local/bin/autopkg run -v ${recipe_list} 2>&1
</span><span class='line'>
</span><span class='line'>  $logger "autopkg initial run to saved log location"
</span><span class='line'>  echo "for this autopkg run, output will not be shown, but rather saved to default log location (${user_home_dir}/Documents/autopkg/autopkg.out"
</span><span class='line'>  /usr/local/bin/autopkg run ${recipe_list} 2>&1 > "${user_home_dir}"/Documents/autopkg/autopkg.out
</span><span class='line'>
</span><span class='line'>  $logger "finished autopkg"
</span><span class='line'>
</span><span class='line'>else
</span><span class='line'>  # default is to just run autopkg and email log if something changed from normal
</span><span class='line'>  $logger "starting autopkg"
</span><span class='line'>  /usr/local/bin/autopkg run ${recipe_list} 2>&1 > /tmp/autopkg.out
</span><span class='line'>
</span><span class='line'>  $logger "finished autopkg"
</span><span class='line'>
</span><span class='line'>  # check output against the saved log and if differences exist, send current log to specified recipient
</span><span class='line'>  if [ "`diff /tmp/autopkg.out \"${user_home_dir}\"/Documents/autopkg/autopkg.out`" != "" ]; then
</span><span class='line'>    # there are differences from a "Nothing downloaded, packaged or imported" run... might be an update or an error
</span><span class='line'>    $logger "sending autopkg log"
</span><span class='line'>    /usr/bin/mail -s "autopkg log" ${mail_recipient}  &lt; /tmp/autopkg.out
</span><span class='line'>    $logger "sent autopkg log to {$mail_recipient}, `wc -l /tmp/autopkg.out | awk '{ print $1 }'` lines in log"
</span><span class='line'>  else
</span><span class='line'>    $logger "autopkg did nothing, so not sending log"
</span><span class='line'>  fi
</span><span class='line'>fi
</span><span class='line'>exit 0</span></code></pre></td></tr></table></div></figure></notextile></div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Slides from 1400+ computers in 3 weeks? Are you nuts?!?]]></title>
    <link href="http://seankaiser.com/blog/2013/05/24/slides-from-1400-plus-computers-in-3-weeks-are-you-nuts/"/>
    <updated>2013-05-24T01:09:00-04:00</updated>
    <id>http://seankaiser.com/blog/2013/05/24/slides-from-1400-plus-computers-in-3-weeks-are-you-nuts</id>
    <content type="html"><![CDATA[<h2>Slides from my presentation at PSU Mac Admins Conference 2013</h2>

<p>If you were in my presentation, or even if you weren't, and would like a copy of my slides, I've posted them below.</p>

<p>As a reminder, I'll have a <a href="http://seankaiser.com/redirects/psumacadmins2013/">blog post</a> in the next couple of days with links to the various sources of information that I used to build our deployment system, as well as some expanded notes on why I designed things the way I did.</p>

<p>I'm offering the slides in two versions: <a href="http://bit.ly/16dsnR3">PDF</a> and <a href="http://bit.ly/13OX4bD">keynote file</a> (in case you want the multisite reposado demo video mainly... or in case you really liked the transitions.)</p>

<p>More information on <a href="http://seankaiser.com/blog/2013/05/23/multi-site-reposado/">multi-site reposado</a> is also available.</p>

<p>If you attended my session, thank you. I hope that I offered something that will help you deploy your own modular deployment system. If you have any questions, you can reach me via twitter <a href="https://twitter.com/seankaiser">@seankaiser</a>. Or, if you would like to contact me via email, my email is my first name -at- seankaiser.com.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Multi-site reposado]]></title>
    <link href="http://seankaiser.com/blog/2013/05/23/multi-site-reposado/"/>
    <updated>2013-05-23T09:23:00-04:00</updated>
    <id>http://seankaiser.com/blog/2013/05/23/multi-site-reposado</id>
    <content type="html"><![CDATA[<h2>A conundrum</h2>

<p>Let's say you work in an environment where you're running <a href="https://github.com/wdas/reposado">reposado</a>. Let's also say that your environment consists of several locations with relatively slow WAN links between them. Additionally, let's say that some of your users roam between locations, and before they move, they just put their MacBooks (or Airs or Pros) to sleep instead of shutting down (because who shuts their machine down every time they're not using their machine?)</p>

<p>In an ideal world, you want to point the machine to the reposado server, but you don't want the machine to download updates over the slow WAN link, and while you could run a reposado server at each location, but by configuring the machine to look at an onsite reposado server, the machine will likely move to another location before softwareupdate checks for updates.</p>

<p>You're running <a href="https://code.google.com/p/munki/">munki</a> and have it set to install Apple software updates? Awesome. You could set the appropriate <code>CatalogURL</code> in your <code>preflight</code> script, but that means that you have to maintain catalog files on several reposado servers, and who wants to do that? (Ok, you could just clone the master reposado server, including the catalog files to get around that last part.)</p>

<p>But what happens if the user has the ability to install Apple software updates via Software Update from the Apple menu (or by running <code>softwareupdate</code> itself)? Their machine might have their previous location's <code>CatalogURL</code> set...</p>

<h2>What do you do?</h2>

<p>Since <code>/Library/Preferences/com.apple.SoftwareUpdate.plist</code> doesn't allow you to configure a <code>PkgURL</code> like munki does, everything goes to the server that the catalog file defined by <code>CatalogURL</code> goes to. But that's the problem.</p>

<p>The workaround? You set up redirects on the master reposado server based on the client's IP address. It seems simple, but I haven't found any references to anyone else doing this. Interested? Great. Let's set it up.</p>

<!-- more -->


<h2>The setup</h2>

<p>First of all, if you're going to get this working, you're going to have to clone your reposado server to a server at your different locations. Just copy the <code>reposado/html/content</code> folder to the other server(s) and set up apache on that server to point to the <code>repsado/html</code> folder as the root folder for the site.</p>

<p>I'm going to assume that you have probably already enabled <code>mod_rewrite</code> to handle the <a href="https://github.com/wdas/reposado/blob/master/docs/URL_rewrites.txt">.sucatalog redirects</a> so you can set one <code>CatalogURL</code> regardless of what OS the client machine is running. If you haven't done that yet, I'll wait for you to go do it. It's that awesome. Seriously.</p>

<p>Once you've got <code>mod_rewrite</code> enabled and your <code>.htaccess</code> file in place (in <code>reposado/html</code>), you need to configure the redirects for your different locations. Using a tool like Google's <a href="https://github.com/wdas/reposado/blob/master/docs/URL_rewrites.txt">IP address range tool</a>, you can build your regular expression rules. You then copy those regular expressions into your <code>.htaccess</code> file and it looks something like this:</p>

<p><div><script src='https://gist.github.com/5636281.js'></script>
<noscript><pre><code></code></pre></noscript></div>
</p>

<p>Seems simple, right? It is. It's just a different way of thinking about things. And it resolves issues related to using different mechanisms to run software update.</p>

<p>(As an alternative, you could probably set up <code>crankd</code>, which is part of the <a href="https://code.google.com/p/pymacadmin/">pymacadmin</a> project, to reconfigure your <code>CatalogURL</code> when the machine wakes from sleep or changes networks, but since I haven't set up <code>crankd</code> yet, I can't offer any guidance on that.)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Big Project- Preparing the machines for deployment with DeployStudio]]></title>
    <link href="http://seankaiser.com/blog/2013/01/31/the-big-project-preparing-the-machines-for-de/"/>
    <updated>2013-01-31T06:00:00-05:00</updated>
    <id>http://seankaiser.com/blog/2013/01/31/the-big-project-preparing-the-machines-for-de</id>
    <content type="html"><![CDATA[<h2>Previously on "The Big Project"</h2>

<p>In <a href="/blog/2013/01/15/were-doing-what-and-on-what-timeline/">part 1</a> of this series, I provided an overview of what I now call "The Big Project." <a href="/blog/2013/01/22/the-big-project-setting-the-stage/">Part 2</a> talked about the importance of inventorying. This article is the first of a series of more detailed technical articles describing various aspects and the tools we used to pull off this project. FIrst up...</p>

<h2>DeployStudio</h2>

<p>We've been using DeployStudio for several years to help us image machines. In the (relatively distant) past, we restored full monolithic images to machines, new or old. Over the past almost two years, we've switched to the thin imaging model. Thin imaging is a method of preserving the contents of a machine's hard drive and deploying either site specific applications or deployment tools to facilitate the installation of such applications and other files in an effort to minimize the amount of time needed to get a machine ready for a user. In our case, we deploy our deployment tools (puppet and munki) and some basic settings. This cuts down on the deployment cost, in both time and bits traveling across the network.</p>

<p>In situations where we need to reimage a machine, we have a workflow that lays down an InstaDMG created vanilla image and then runs the normal thin imaging workflow.</p>

<!-- more -->


<h2>The thin imaging workflow</h2>

<p>When we "Northmont-ize a new machine" (the name of our thin imaging workflow), DeployStudio performs the initial basic setup fo our machines. Everything is rather basic at this point. The workflow does the following:</p>

<ul>
<li>set the time server to our internal NTP server. (This is the only non-automated step in the workflow. I didn't automate this step to let the technician confirm that the correct drive is selected as the target volume. All other steps are automated and use the previous task target option for the target volume.)</li>
<li>set the computer's name via a query to our help desk</li>
<li>perform an anonymous bind the machine to our open directory server</li>
<li>install puppet, facter, a basic puppet.conf file, a tweaked ruby puppet wrapper that I copied from Gary Larizza way back when he was still in the K12 education space at Huron, and a launchdaemon to keep things running on a schedule</li>
<li>create our default local user set (the users are actually payloadless packages created from InstaDMG's <a href="http://code.google.com/p/instadmg/source/browse/trunk/AddOns/createUser/archived/CreateLionUser-README.txt">CreateLionUser</a>.) We create two admin accounts, one for the district tech staff to use and one for the in-building tech support staff to use. We also have a passwordless "classroom" account so that users can access the local machine in case the network or server (either file server or open directory infrastructure) is down.</li>
<li>install munki</li>
<li>skip Apple's setup assistant that runs for new machines and enable the ARD agent</li>
<li>set munki's client_identifier via a query to our help desk</li>
<li>set softwareupdate's CatalogURL to point to the central reposado server and, based on information gathered via a query to our help desk, configure it to use the test catalog</li>
<li>join the computer to the appropriate wifi network as specified in the help desk (if one is specified. Since a majority our new computers are MacBook Pros, they all have a wifi network specified. The new iMacs and most of the relocated machines are all hardwired, so they don't have a network specified, and the script just exits appropriately.)</li>
<li>perform a non-destructive partition of the hard drive into two partitions (after checking that the drive only has one partition.) The first partition is set to 250GB, with the second partition using the remaining space (which has to be calculated since the diskutil command doesn't know how to "use remaining space".) The script then grabs the UUID for the new second partition and adds it to /etc/fstab to be mounted at /Users (after checking that this information isn't already in the file.) It also creates a new Shared folder to match the one that's in the /Users folder by default on an OS X machine. Having this partition in place allows the users of the machine to have a space that's safe from future reimaging. Our teaching staff are using portable home directories (I know... bad word) so their local synchronized home folder sits on this "safe" partition.</li>
<li>touches /Users/Shared/.com.googlecode.munki.checkandinstallatstartup to <a href="http://code.google.com/p/munki/wiki/BootstrappingWithMunki">bootstrap munki</a> (make munki run immediately upon boot instead of on its regular schedule.)</li>
<li>reboot the machine (I have a reboot step because I've configured DeployStudio not to reboot after a workflow runs so we can run multiple workflows if necessary. The drawback of this is that computers never appear to finish in the Activity area of DeployStudio.)</li>
</ul>


<p>All of our workflows run as "postponed installations". Is this necessary? For some steps, such as installing packages, it seems to work better. Since I'm doing a postponed install for some things, I decided to do it for all steps.</p>

<h2>Full reimage workflow</h2>

<p>If we do actually deploy a new image, it's usually because something has gone wrong on the machine or we've simply replaced the hard drive. I've used InstaDMG to create a vanila 10.6, 10.7, and 10.8 image. Since we have 6 elementary schools, a middle school, a high school, a service center (where my department's offices are), and the administrative office, each with their own LAN and only a 25 mbps WAN link back to the datacenter, I've implemented a modified version of a strategy I discovered a couple of years ago to have a <a href="http://www.deploystudio.com/Forums/viewtopic.php?pid=2614">central DeployStudio server with local images repositories</a>. (I know DeployStudio now offers a synchronization process, but I've never looked into it. This process works well for us. And, remember from part 1, one of the steps I had to complete to start this project was to update DeployStudio and its NetBoot image... I doubt that prior to a month and a half ago that the version we were running offered synchronization.)</p>

<p>So, our complete reimage workflow looks like this:</p>

<ul>
<li>run the local repository mount script</li>
<li>restore the vanilla image</li>
<li>run the northmont-ize workflow described above</li>
</ul>


<h2>The logistics of prepping 1200+ machines</h2>

<p>We wanted our machines to be usable as soon as we deployed them so as to cut down on the amount of time we were in the schools. (In one case, we actually replaced a school's computers as the students cycled through lunch, which at that school's was about an hour and a half.)</p>

<p>Since our offices are on the second floor in our building, and although our server room has a garage door that allows our warehouseman to forklift pallets of equipment through, we had no intention of moving 240+ 5-pack boxes of MacBook Pros any more than we really needed to. We commandeered a couple of tables near the staff mailboxes downstairs between our storage "cage" and the warehouse, dropped a couple of extension cords and a patch cord (offering a gigabit connection) from the office space above, and set up an imaging bench with an HP switch, two power strips, and 10 Magsafe power adapters. One of my coworkers had the idea of taping the power and patch cords to the tables so they didn't slide around. Initially I wasn't sure this was necessary, but it turned out to be brilliant.</p>

<p>Since we could get 10 computers going at a time, and get those 10 computers booted into DeployStudio, run the workflow, reboot to go through the firstboot process, reboot a second time, let puppet and munki run to install our settings and applications (which will be detailed in subsequent articles), then shut the computers down to rebox them, all in about 15-20 minutes per group. While one batch was running, we'd unbox the next batch and have them ready to move into place when the previous batch was finished.</p>

<p><strong>Think about that... every 15-20 minutes, we had 10 computers ready to be deployed, with our applications, our users, our settings, etc.</strong> Repeat that process 120+ times, and you can understand how I was able to successfully Northmont-ize an iMac whose display was DOA. After a while, I could do that process in my sleep, knowing about how long to wait for each step of the process, how many down arrows to press to get to our workflow, etc. I did get a bit excited (probably overly so) when we heard the iMac reboot after the workflow ran, then reboot again a couple of minutes later when the first boot processes finished.</p>

<p>Another benefit of thin imaging and using tools like puppet and munki is that as things change, things need to be added, etc., you've got a system in place that will handle that, automatically. You don't need to rebuild your image.</p>

<h2>What's coming up</h2>

<p>I'll have dedicated articles on our puppet and munki configurations, a brief discussion on how I'm using luggage to package printer installers and repackage applications that don't come with good installers, some tips on inventorying machines, and some scripts that I've written to tie everything together, and finally reposado, in that general order.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Big Project- Setting the Stage by Inventorying]]></title>
    <link href="http://seankaiser.com/blog/2013/01/22/the-big-project-setting-the-stage/"/>
    <updated>2013-01-22T07:37:00-05:00</updated>
    <id>http://seankaiser.com/blog/2013/01/22/the-big-project-setting-the-stage</id>
    <content type="html"><![CDATA[<h2>A small detour</h2>

<p>I was going to keep this article for near the end of the series, but then it might imply that the prep work wasn't important. This is wrong. It's very important, and in our case just as critical as the deployment tools themselves. I'd guess this is (or should be) the case everywhere.</p>

<h2>Why is inventorying so important?</h2>

<p>Any time you deploy a machine, whether it's one machine or (in our case) part of a 1300+ machine deployment, you need to add the machine to your inventory system. In our case, our inventory system is our help desk (we run Web Help Desk.) As I'll describe in later articles, many of the processes in our deployment workflow refer to the help desk and custom asset fields so we can have a dynamic configuration without having to edit files on individual machines.</p>

<!-- more -->


<h2>How we did it</h2>

<p>Before we started this project, I asked my Twitter followers if anyone had <a href="https://twitter.com/seankaiser/status/264299709722656768">recommendations for barcode scanners</a>. I got a couple of responses that folks used the Symbol barcode scanner that is recommended for use with Apple's GSX (it's the <a href="http://www.motorola.com/Business/US-EN/Business+Product+and+Services/Bar+Code+Scanning/General+Purpose+Scanners/DS6707-DP_US-EN">Symbol DS6707</a> if you're interested.) We immediately bought this scanner because, well, we had no intention of manually entering serial numbers or ethernet and wifi addresses into the help desk. This guaranteed that there were no typos as well. Typos are bad when you depend on the data that the OS gives you to look up settings. Also, if you've ever looked at an Apple barcode, it's sometimes hard to differentiate a B from an 8, or an S from a 5. But a barcode scanner doesn't have this problem. If you don't have a barcode scanner and have trouble reading Apple's 4 point font, invest in one.</p>

<p>Using the exported asset template from Web Help Desk, one of my coworkers created an import file that had all of the information we intended to import (asset numbers, room numbers, building, assigned client, etc.), except for the information from the barcodes. Two of our other coworkers handled scanning the boxes (thank you Apple for putting the three pieces of information on the outside of the MacBook Pro 5 pack boxes), applying the inventory asset tags to the machines, and labeling the boxes what asset numbers were in the box. Once they had scanned all of the machines for an individual building, they forwarded the file to me. I cleaned up some of the stuff (took off the leading S from the scanned serial number, removing spaces and converting to lower case the ethernet and wifi addresses mainly) and imported the file into Web Help Desk.</p>

<h2>Next step</h2>

<p>Now we were ready to start "imaging" the machines in DeployStudio. That's a topic worthy of its own article.</p>
]]></content>
  </entry>
  
</feed>
